{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from itertools import cycle\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## function 1 method that takes as finds and counts unique occurrances of a given column within a csv file\n",
    "# @param[in] infilename: name of csv file\n",
    "# @param[in] col: name of column of interest\n",
    "# @param[in] outFile: exported file showing unique occurances in column \"col\" as well as how many times they have occured\n",
    "from numpy import ceil\n",
    "\n",
    "\n",
    "def exportUniqueValuesInColumn(infilename, col,outFile):   \n",
    "    df = pd.read_csv(infilename, encoding='latin-1',dtype=\"string\")\n",
    "    dfgroup = df.groupby(col).size() #df[col].unique()\n",
    "    dfgroup.to_csv(outFile)\n",
    "    print (\"EXIT SUCCESS\")\n",
    "\n",
    "\n",
    "def exportUniqueValuesInColumnOfPlots(infilename, colSpeciesAll, colPoltsS, infilename_nleve, colPoltsP, outFile_nleve):\n",
    "    # Read the files\n",
    "    df_all = pd.read_csv(infilename, encoding='latin-1')\n",
    "    df_nleve = pd.read_csv(infilename_nleve)\n",
    "    \n",
    "    # Merge the two dataframes based on plot ID\n",
    "    merged_df = pd.merge(df_all, df_nleve[[colPoltsP]], how='inner', left_on=colPoltsS, right_on=colPoltsP)\n",
    "    \n",
    "    # Group by species and count occurrences\n",
    "    species_count = merged_df.groupby(colSpeciesAll).size().reset_index(name='Count')\n",
    "    \n",
    "    # Write to output file\n",
    "    species_count.to_csv(outFile_nleve, index=False)    \n",
    "    print(\"File exported in \", outFile_nleve)\n",
    "\n",
    "## function 1.5, finds if plots in tree csv really exist and then counts how many trees exist for each specie/genus\n",
    "def groupAndExportCountToCSV(dfinput, col1, col2, plotsFile, plotsIDinPlotsFile, csv_filename): \n",
    "    dfplots = pd.read_csv(plotsFile, encoding='latin-1', dtype=str)\n",
    "    existingPlots = dfplots[plotsIDinPlotsFile].tolist()\n",
    "    df = pd.read_csv(dfinput, encoding='latin-1', dtype=\"string\")\n",
    "    df = df[df[col1].isin(existingPlots)] # col1 = colPlots\n",
    "    grouped = df.groupby([col1, col2])\n",
    "    aggregated = grouped.size().reset_index(name='Count')\n",
    "    aggregated.to_csv(csv_filename, index=False)\n",
    "\n",
    "    aggregated_summed = aggregated.groupby(col2)['Count'].sum().reset_index(name='TotalCount')\n",
    "    aggregated_summed.to_csv(csv_filename, index=False)\n",
    "\n",
    "\n",
    "## function 1.7, finds if plots in tree csv really exist and then counts how many trees exist for each specie/genus\n",
    "def readFilesAndExportCountPerClassToCSV(inDir, outCsv):\n",
    "    result_df = pd.DataFrame(columns=['Class', 'TotalCount'])\n",
    "\n",
    "    for file_name in os.listdir(inDir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            full_file_path = os.path.join(inDir, file_name)\n",
    "            current_df = pd.read_csv(full_file_path)\n",
    "            class_name = file_name.replace('.csv', '')\n",
    "            line_count = len(current_df) \n",
    "            result_df = result_df.append({'Class': class_name, 'LineCount': line_count}, ignore_index=True)\n",
    "    result_df.to_csv(outCsv, index=False)\n",
    "\n",
    "\n",
    "## functions 2 finds number of occurances of tree species per plot \n",
    "# @param[in] inCSVPlotsFile: name of csv file containing plot data\n",
    "# @param[in] colPlots: name of column containing the IDs of the plots\n",
    "# @parma[in] colSpecies: name of column containing the type of species\n",
    "# @param[in] outFile: name of csv file exported with dominant species per plot\n",
    "#  param[in] plotsFile the csv file contianing the plot information (used to check if the plots noted in inCsv exist)\n",
    "#  param[in] plotsIDinPlotsFile the column in plotsFile containing the PlotID information\n",
    "def exportPlotsWithDominantSpecies(inCsvPlotsFile, colPlots,plotsFile, plotsIDinPlotsFile, colSpecies, outFile):\n",
    "    #labelsGeneral = [\"Pinus\",\"Quercus\",\"Fagus\",\"Eucalyptus\",\"Castanea\",\"Juniperus\",\"Betula\",\"Populus\",\"Fraxinus\",\"Alnus\",\"Abies\",\"Myrica\",\"Arbutus\",\"Acer\",\"Ilex\",\"Salix\",\"Pseudotsuga\",\"Laurus\",\"Crataegus\",\"Sorbus\",\"Corylus\",\"Persea\",\"Acacia\",\"Larix\",\"Prunus\",\"Chamaecyparis\",\"Olea\",\"Tilia\",\"Robinia\",\"Platanus\",\"Cupressus\",\"Picea\",\"Ulmus\",\"Cedrus\",\"Juglans\",\"Phillyrea\",\"Pyrus\",\"Taxus\",\"Otras\",\"Phoenix\",\"Malus\",\"Otros\",\"Sambucus\",\"Tamarix\",\"Celtis\",\"Picconia\",\"Ficus\",\"Apollonias\",\"Heberdenia\",\"Ceratonia\",\"Pinus\",\"Quercus\",\"Fagus\",\"Eucalyptus\",\"Castanea\",\"Juniperus\",\"Betula\",\"Populus\",\"Fraxinus\",\"Alnus\",\"Abies\",\"Myrica\",\"Arbutus\",\"Acer\",\"Ilex\",\"Salix\",\"Pseudotsuga\",\"Laurus\",\"Crataegus\",\"Sorbus\",\"Corylus\",\"Persea\",\"Acacia\",\"Larix\",\"Prunus\",\"Chamaecyparis\",\"Olea\",\"Tilia\",\"Robinia\",\"Platanus\",\"Cupressus\",\"Picea\",\"Ulmus\",\"Cedrus\",\"Juglans\",\"Phillyrea\",\"Pyrus\",\"Taxus\",\"Otras\",\"Phoenix\",\"Malus\",\"Otros\",\"Sambucus\",\"Tamarix\",\"Celtis\",\"Picconia\",\"Ficus\",\"Apollonias\",\"Heberdenia\",\"Ceratonia\"]\n",
    "    \n",
    "    dfplots = pd.read_csv(plotsFile, encoding='latin-1', dtype=str)\n",
    "    existingPlots = dfplots[plotsIDinPlotsFile].tolist()\n",
    "    df = pd.read_csv(inCsvPlotsFile, encoding='latin-1', dtype=\"string\")\n",
    "    df = df[df[colPlots].isin(existingPlots)]\n",
    "    df_counts = df.groupby([colPlots, colSpecies]).size().reset_index(name='Count')\n",
    "    df_pivot = df_counts.pivot(index=colPlots, columns=colSpecies, values='Count').fillna(0)\n",
    "    #df_pivot = df_pivot[labelsGeneral]\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    #df_pivot = df_pivot[labelsGeneral]\n",
    "    df_pivot.to_csv(outFile, index=False)\n",
    "    print(\"File saved in: \", outFile)\n",
    "\n",
    "\n",
    "## function 3 takes as input the output of function 2 and adds a column name \"sum\", which\n",
    "#  contains the sum of trees per plot and then calculates the percentage of each specie/genera per plot\n",
    "def getPercentageOfSpeciesPerPlot(inCsv, outCsv):\n",
    "    df = pd.read_csv(inCsv, encoding='latin-1', low_memory=False)\n",
    "    labels = list(df.columns)\n",
    "    print(labels)\n",
    "    count = 1\n",
    "    df['sumOfAllTrees'] = df[df.columns[1:]].sum(axis=1)\n",
    "    \"\"\"\n",
    "    species_columns = df.columns[1:]  # Assuming the first column is 'PlotID'\n",
    "    for species in species_columns:\n",
    "        percentage_column_name = f'{species}_per'\n",
    "        df[percentage_column_name] = df[species] / df['sumOfAllTrees']\n",
    "    \n",
    "    df.to_csv(outCsv, index=False)\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    species_columns = df_copy.columns[1:]\n",
    "    for species in species_columns:\n",
    "        df_copy[species] = df[species] / df_copy['sumOfAllTrees']\n",
    "\n",
    "       \n",
    "    df_copy.to_csv(outCsv, index=False)\n",
    "\n",
    "    print(\"   *** getPercentageOfSpeciesPerPlot: exit success!   ***\")\n",
    "\n",
    "\n",
    "## function 4 takes as input the output of function 3 and creates a folder for each class\n",
    "#  within the folder it stores three files: (1) all the plots that contain only this class,\n",
    "#  (2) all the plots that contain 75% and more than the specified class but less than 100%\n",
    "#  (3) all the plots that contain 50-75% of this class\n",
    "#  param[in] inCsv the output of Function 3 containing the how many of each tree class exist in each plot in percentages\n",
    "#  param[in] colPlots the column in inCsv defining the Plot ID \n",
    "#  param[in] outDir the directory where the new folders will be stored\n",
    "def getPlotIDsPerClass(inCsv, colPlots, outDir):\n",
    "    df = pd.read_csv(inCsv, encoding='latin-1', low_memory=False)\n",
    "    if 'sumOfAllTrees' in df.columns:\n",
    "        df = df.drop(columns=['sumOfAllTrees'])\n",
    "    \n",
    "    df.set_index(colPlots, inplace=True)\n",
    "    for species_column in df.columns:\n",
    "        filtered_df100 = df[(df[species_column] >= 0.99999999999) ]\n",
    "        filtered_df75  = df[(df[species_column] >= 0.75         ) ] #& (df[species_column] < 0.99999999999)]\n",
    "        filtered_df50  = df[(df[species_column] >= 0.5          ) ] #& (df[species_column] < 0.75         )]\n",
    "        \n",
    "        selected_plots100 = filtered_df100.index.tolist()\n",
    "        selected_plots75  = filtered_df75.index.tolist()\n",
    "        selected_plots50  = filtered_df50.index.tolist()\n",
    "\n",
    "        outDir75  = os.path.join(outDir.rstrip(os.path.sep), \"100\" )\n",
    "        outDir50  = os.path.join(outDir.rstrip(os.path.sep), \"50\" )\n",
    "        outDir100  = os.path.join(outDir.rstrip(os.path.sep), \"100\")\n",
    "        if not os.path.exists(outDir75):\n",
    "            os.makedirs(outDir75)\n",
    "        if not os.path.exists(outDir50):\n",
    "            os.makedirs(outDir50)\n",
    "        if not os.path.exists(outDir100):\n",
    "            os.makedirs(outDir100)\n",
    "        \n",
    "        \n",
    "        selected_plots_df = pd.DataFrame({species_column: selected_plots75})\n",
    "        output_file75 =os.path.join(outDir75, f\"{species_column}.csv\")\n",
    "        selected_plots_df.to_csv(output_file75, index=False)\n",
    "\n",
    "        selected_plots_df = pd.DataFrame({species_column: selected_plots50})\n",
    "        output_file50 =os.path.join(outDir50, f\"{species_column}.csv\")\n",
    "        selected_plots_df.to_csv(output_file50, index=False)\n",
    "\n",
    "        selected_plots_df = pd.DataFrame({species_column: selected_plots100})\n",
    "        output_file100 =os.path.join(outDir100, f\"{species_column}.csv\")\n",
    "        selected_plots_df.to_csv(output_file100, index=False)\n",
    "\n",
    "        print(\"files exported in directories: \\n\", outDir50, \"\\n\", outDir75, \"\\n\", outDir100)\n",
    "\n",
    "\n",
    "## Function 5 used to merge Quercus - takes as input multiple csv files and merges them renaming the first column (label) \n",
    "# - should work with multiple columns, just give us input the new labels in comma separated\n",
    "def mergeCsvFiles(directory_path, output_file, new_column_label):\n",
    "    files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "    with open(output_file, 'w') as combined_file:\n",
    "        combined_file.write(new_column_label+\"\\n\")\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(directory_path, file_name)         \n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()[1:]\n",
    "                combined_file.writelines(lines)\n",
    "    print(\"New File saved in \", output_file)\n",
    "    \n",
    "\n",
    "\n",
    "## Function 6, takes as input a dirctory and reads all the csv files within that directory \n",
    "# and counts how many columns exist in those file minus one (for the label)\n",
    "def exportCsvSummary(directoryPath, outputFile):\n",
    "    files = [file for file in os.listdir(directoryPath) if file.endswith('.csv')]\n",
    "    file_names = []\n",
    "    row_counts = []\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directoryPath, file_name)\n",
    "        file_names.append(os.path.splitext(file_name)[0])\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()[1:]\n",
    "            row_count = len(lines)\n",
    "            row_counts.append(row_count)\n",
    "\n",
    "    data = {'File Name': file_names, 'Row Count': row_counts}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(outputFile, index=False)\n",
    "\n",
    "\n",
    "## Function 7 takes asn input two csv files (1) the list of the plots per genus/species\n",
    "# (2) the csv with the features e.g., NDVI time-series with the plotDID data\n",
    "# exports a subset of the 2nd file, only the rows whose PlotID is listed in the first file\n",
    "def filter_rows_by_plot_ids(directoryPath, second_file_path, colPlotID, outFolder):\n",
    "    if directoryPath == outFolder:\n",
    "        raise ValueError(\"Input and Outpuf files must be different!\")\n",
    "    # else\n",
    "        # input and output files are different so files will not be overwritten\n",
    "\n",
    "    files = [file for file in os.listdir(directoryPath) if file.endswith('.csv')]\n",
    "    # PlotIDs with features \n",
    "    data_df = pd.read_csv(second_file_path,dtype=str)\n",
    "\n",
    "    if not os.path.exists(outFolder):\n",
    "            os.makedirs(outFolder)\n",
    "\n",
    "    for file_name in files:\n",
    "        full_file_path = os.path.join(directoryPath, file_name)\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            lines = [line.strip() for line in file.readlines()[1:]]\n",
    "            flat_lines = [plot_id for line in lines for plot_id in line.split(\",\")]\n",
    "        filtered_data_df = data_df[data_df[colPlotID].isin(lines)]      \n",
    "        new_file_path = os.path.join(outFolder, file_name)\n",
    "        filtered_data_df.to_csv(new_file_path, index=False)\n",
    "    print(\"Files saved in \", outFolder)\n",
    "\n",
    "\n",
    "## function 8 selects samples for training \n",
    "#  @param[in] inDir takes as input a directory with the .csv files containing the NDVI time-series and the plotIDs - each file has the name of the class(e.g. genus) +.csv \n",
    "#  @param[in] classList is a list of the classes of interested\n",
    "#  @param[in] noOfSamples number of random samples to select per class of interest\n",
    "#  @param[in] outCsv exports one csv file that contains all the randomly selected samples with their features (e.g., NDVI time-series) and the column \"class\" is store the class (e.g., genus) each row belongs to \n",
    "def selectSamples(inDir,classList,noOfSamples,outCsv):\n",
    "    files = [file for file in os.listdir(inDir) if file.endswith('.csv')]\n",
    "    csvClassListCsv = [class_name + '.csv' for class_name in classList]\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file_name in files:\n",
    "        if file_name in csvClassListCsv:\n",
    "            full_file_path = os.path.join(inDir, file_name)\n",
    "            current_df = pd.read_csv(full_file_path)\n",
    "            current_df['class'] = file_name.replace('.csv', '')\n",
    "            if current_df['class'].iloc[0] not in classList:\n",
    "                warnings.warn(f\"Warning: Class {current_df['class'].iloc[0]} not found in classList.\")\n",
    "            print(file_name)\n",
    "            sampled_df = current_df.sample(n=noOfSamples, replace=False, random_state=42)  \n",
    "            if len(sampled_df) < noOfSamples:\n",
    "                warnings.warn(f\"Warning: Class {current_df['class'].iloc[0]} has less than {noOfSamples} rows.\")\n",
    "\n",
    "            combined_df = pd.concat([combined_df, sampled_df], ignore_index=True)\n",
    "    combined_df.to_csv(outCsv, index=False)\n",
    "    print(\"File exported in \", outCsv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## function 9 adds co-ordinates to the csv files according to the plotFile\n",
    "def addCoordinates(inFile, plotsFile, plotsIDinPlotsFile, CX, CY, outfile):\n",
    "    in_df = pd.read_csv(inFile)\n",
    "    plots_df = pd.read_csv(plotsFile)\n",
    "\n",
    "    if CX not in plots_df.columns or CY not in plots_df.columns:\n",
    "        raise ValueError(f\"Columns {CX} and {CY} not found in {plotsFile}\")\n",
    "\n",
    "    in_df[CX] = None\n",
    "    in_df[CY] = None\n",
    "\n",
    "    for index, row in in_df.iterrows():\n",
    "        plot_id = row[plotsIDinPlotsFile]\n",
    "        match_row = plots_df[plots_df[plotsIDinPlotsFile] == plot_id]\n",
    "\n",
    "        if not match_row.empty:\n",
    "            in_df.at[index, CX] = match_row[CX].iloc[0]\n",
    "            in_df.at[index, CY] = match_row[CY].iloc[0]\n",
    "\n",
    "    in_df.to_csv(outfile, index=False)    \n",
    "\n",
    "    print (\"New file exported in \", outfile)\n",
    "\n",
    "## function 10 takes as input a file and creates multiple subfiles according to a given class\n",
    "def divideToSubfiles(inFile, classCol, outDir):\n",
    "    df = pd.read_csv(inFile)\n",
    "\n",
    "    unique_classes = df[classCol].unique()\n",
    "\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "\n",
    "    for class_value in unique_classes:\n",
    "        subset_df = df[df[classCol] == class_value]\n",
    "\n",
    "        out_file = os.path.join(outDir, f\"{class_value}.csv\")\n",
    "        subset_df.to_csv(out_file, index=False)\n",
    "    print(\"Subfiles exported in \", outDir)\n",
    "\n",
    "\n",
    "## function 11 randomly divides my test data into three datasets for cross validation \n",
    "def divideToThreeForCrossValidation(inCsv, outDir):\n",
    "    df = pd.read_csv(inCsv)\n",
    "    unique_classes = df['class'].unique()\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "\n",
    "    df1 = pd.DataFrame(columns=df.columns)\n",
    "    df2 = pd.DataFrame(columns=df.columns)\n",
    "    df3 = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for unique_class in unique_classes:\n",
    "        class_df = df[df['class'] == unique_class]\n",
    "\n",
    "        for index, row in class_df.iterrows():\n",
    "            count = index  \n",
    "            if count % 3 == 0:\n",
    "                df1 = df1.append(row)\n",
    "            elif count % 3 == 1:\n",
    "                df2 = df2.append(row)\n",
    "            else:\n",
    "                df3 = df3.append(row)\n",
    "\n",
    "    df1.to_csv(os.path.join(outDir, 'file_1.csv'), index=False)\n",
    "    df2.to_csv(os.path.join(outDir, 'file_2.csv'), index=False)\n",
    "    df3.to_csv(os.path.join(outDir, 'file_3.csv'), index=False)\n",
    "\n",
    "\n",
    "## function 12, calculates KNN for the NDVI time-series\n",
    "def classifyKNN (train1, train2, test1, test1_results, k):\n",
    "    df1 = pd.read_csv(train1)\n",
    "    df2 = pd.read_csv(train2)\n",
    "    trainingData = pd.concat([df1, df2], ignore_index=True)\n",
    "    testingData = pd.read_csv(test1)\n",
    "\n",
    "    labels_of_interest = [\"0_NDVI\", \"1_NDVI\", \"2_NDVI\", \"3_NDVI\", \"4_NDVI\", \"5_NDVI\", \"6_NDVI\", \"7_NDVI\", \"8_NDVI\", \"9_NDVI\", \"10_NDVI\", \"11_NDVI\"]\n",
    "    class_column = \"class\"\n",
    "\n",
    "\n",
    "\n",
    "    for test_index, test_row in testingData.iterrows():\n",
    "        testRow = test_row[labels_of_interest].tolist()\n",
    "        tmp_df = pd.DataFrame(columns=[class_column, \"distance\"])\n",
    "\n",
    "        for train_index, train_row in trainingData.iterrows():\n",
    "            trainRow = train_row[labels_of_interest].tolist()\n",
    "            countNonNull = 0.0\n",
    "            distance = 0.0\n",
    "\n",
    "            for i in range(1, len(trainRow)):\n",
    "                if pd.notna(trainRow[i]) and pd.notna(testRow[i]):\n",
    "                    countNonNull += 1.0\n",
    "                    distance += math.sqrt((trainRow[i] - testRow[i])**2)\n",
    "\n",
    "            if countNonNull > 0.1:\n",
    "                distance /= countNonNull\n",
    "\n",
    "            tmp_df = pd.concat([tmp_df, pd.DataFrame({class_column: [train_row[class_column]], \"distance\": [distance]})], ignore_index=True)\n",
    "            \n",
    "        tmp_df = tmp_df.sort_values(by=\"distance\", ascending=True)\n",
    "\n",
    "        tmp_df = tmp_df.head(k)\n",
    "        \n",
    "        for index, row in tmp_df.iterrows():\n",
    "            if row[\"distance\"] <= 0.000000000001:\n",
    "                tmp_df.at[index, \"distance\"] = 10000\n",
    "            else:\n",
    "                tmp_df.at[index, \"distance\"] = 1.0 / row[\"distance\"]\n",
    "\n",
    "        unique_classes = tmp_df[class_column].unique()\n",
    "\n",
    "        summed_distances_df = pd.DataFrame(columns=[\"class\", \"summedDistance\"])\n",
    "\n",
    "        for unique_label in unique_classes:\n",
    "            subset_df = tmp_df[tmp_df[class_column] == unique_label]\n",
    "            sum_distances = subset_df[\"distance\"].sum()\n",
    "            summed_distances_df = pd.concat([summed_distances_df, pd.DataFrame({\"class\": [unique_label], \"summedDistance\": [sum_distances]})], ignore_index=True)\n",
    "        \n",
    "        max_distance_class = summed_distances_df.loc[summed_distances_df[\"summedDistance\"].idxmax()][\"class\"]\n",
    "        testingData.at[test_index, \"Results\"] = max_distance_class\n",
    "\n",
    "    testingData.to_csv(test1_results, index=False)\n",
    "    \n",
    "    #return distances_df\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Classified results are given in \", test1_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### FOREST TYPES #######\n",
    "## method that saves a csv file for each uinique label in column column_name\n",
    "def group_rows_by_label(input_csv, column_name,outDir):\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "    df = pd.read_csv(input_csv)\n",
    "    unique_labels = df[column_name].unique()\n",
    "    for label in unique_labels:\n",
    "        label_df = df[df[column_name] == label]\n",
    "        output_filename =os.path.join(outDir+f\"{label}.csv\")\n",
    "        print(output_filename)\n",
    "        label_df.to_csv(output_filename, index=False)\n",
    "    print(\"files saved in \", outDir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_and_save_metrics(input_file, class_column, results_column, output_file):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    unique_classes = df[class_column].unique()\n",
    "    metrics = []\n",
    "\n",
    "    for class_name in unique_classes:\n",
    "        tp = ((df[class_column] == class_name) & (df[results_column] == class_name)).sum()\n",
    "        tn = ((df[class_column] != class_name) & (df[results_column] != class_name)).sum()\n",
    "        fp = ((df[class_column] != class_name) & (df[results_column] == class_name)).sum()\n",
    "        fn = ((df[class_column] == class_name) & (df[results_column] != class_name)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics.append({\n",
    "            'Class': class_name,\n",
    "            'TP': tp,\n",
    "            'TN': tn,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_score\n",
    "        })\n",
    "\n",
    "    # Create DataFrame from metrics\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    print (metrics_df)\n",
    "    # Save metrics to CSV\n",
    "    metrics_df.to_csv(output_file, index=False)\n",
    "    print (\"File saved in \", output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
